{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdEl6Nx6IVma"
   },
   "source": [
    "# The MLEnd Deception Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaGn4ICrfqXZ"
   },
   "source": [
    "# 1 Author\n",
    "\n",
    "**Student Name**:  Anabia Aijaz <br>\n",
    "**Student ID**:  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o38VQkcdKd6k"
   },
   "source": [
    "# 2 Problem formulation\n",
    "\n",
    "\n",
    "The problem involves developing a predictive model to determine a 30-second audio recording is true or deceptive. This is challenging because it not only involves analysing complex audio signals but also pose issues like background noise and inconsistent recording quality, variability in speakers' accents and styles, limited data, the complexity of extracting reliable acoustic features, and potential biases because of limited diversity in the dataset. However, this project is exciting because it could lead to further research in lie detection models that might be useful in areas like security and law enforcement agencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPTSuaB9L2jU"
   },
   "source": [
    "# 3 Methodology\n",
    "\n",
    "\n",
    "The methodology involves mapping audio features (predictor attributes) to the corresponding labels (true or deceptive) by using classification supervised learning techniques. For this purpose the MLEnd deception dataset is used, which consisit of 100 audio recordings with three attributes namely a complex audio signal, descriptive labels and binary labels. A dataset is also provided with audio file name, language and labels however for this project it was not used.  For our convience First we will split the audio recordings as per the input requirement of 30-sec duration. However, this recordings cannot be fed directly to our model as audio signals are complex datatypes having continuous waveforms that represent sound over time. To convert them into a digital form, they are sampled at a specific rate, called the sampling frequency, which determines how many data points (samples) are recorded per second. Each sample captures the amplitude of the sound wave at a specific moment.\n",
    "\n",
    "Using librosa.load(sr=None), we can get the original audio sampling frequency which is 44,100 Hz, meaning there are 44,100 samples per second. For a 30-second recording, this gives us 1,323,000 samples. If we feed this directly into our model, we would be working with a high-dimensional dataset, which can lead to issues like overfitting due to insufficient samples for training. To manage this, we will be extracting key features after which we perform the training, validation and test task. \n",
    "\n",
    "**Training task:** Trained the model to recognise patterns or relationships from the extracted features of our audio data by adjusting its parameters based on labeled examples in the training dataset. \n",
    "\n",
    "**Validation Task:** Evaluated different model's performance and their hyperparameters on unseen data during training to identify the best-performing model, avoid overfitting and helps in model optimization. \n",
    "\n",
    "Furthermore, K-fold cross-validation is used to reduce reliance on a single validation set. The dataset is divided into 10 folds; the model is trained on 9 folds and tested on the 10th, repeating this process for all folds. This ensures efficient use of available data and unbiased evaluation.\n",
    "\n",
    "**Test Task:** Estimated the deployment quality of a model on unseen data. It is performed only once after building our model to see how the model behaves in real world scenario on unknown data.\n",
    "\n",
    "\n",
    "The model can be evaluated based on the following quality metrics to detect whether a recording is true or deceptive. Since the dataset has true as 0(negative) and deceptive as 1(positive) so the interpretation of the quality metrics are based on that. However, later in our code we displayed the metrics for both classes seperately\n",
    "\n",
    "**Accuracy:** Proportion of correctly classified samples both true and deceptive. <br>\n",
    "**Error Rate:** Proportion of misclassified samples both true and deceptive. It is the complement of accuracy ie the model failure rate so we will not be calculating it seperately for our models. <br>\n",
    "â€‹**Confusion Matrix (Count/Rate):** A table that summarizes the number/rate of true positives(correctly predicted deceptive(1)), true negatives(correctly predicted true(0)), false positives(incorrectly predicted deceptive(1)), and false negatives(incorrectly predicted true(0)) <br>\n",
    "**Recall (Sensitivity):** Proportion of actual deceptive samples correctly classified as deceptive. It helps to minimize the risk of missing a deceptive recording (false negatives) <br>\n",
    "**Specificity:** Proportion of actual true samples correctly classified. It helps assess how well the model avoids incorrectly identifying a true recording as deceptive. <br>\n",
    "**Precision:** Proportion of predicted deceptive samples that are actually deceptive. It helps assess that when the model predicts a recording as deceptive, it is indeed deceptive and not a false alarm <br>\n",
    "**F1 Score:** Harmonic mean of precision and recall, balancing false positives(deceptive) and false negatives(true). <br>\n",
    "\n",
    "**Example Interpretation**\n",
    "For eg in scenarios such as security and law enforcement applications, a high recall means real threats or crimes are identified whereas a high precision ensures that the system is accurate in identifying only real threats or crimes, avoiding wasting resources. However, the high precision might come at the cost of missing some positive cases (low recall), meaning that some true threats or crimes might go undetected. Therefore a F1 Score balances them both in which a High F1-score indicates that it can identify threats or criminal activities without overwhelming the system with false alarms.\n",
    "\n",
    "\n",
    "For our analysis we used Accuracy, Confusion Matrix (Count/Rate), Recall, Precision and F1 Score as the quality metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "N3BwrtEdLDit"
   },
   "source": [
    "# 4 Implemented ML prediction pipelines\n",
    "\n",
    "The ML prediction pipelines involves the following stages:<br>\n",
    "**Input:** Audio recording of varying duration from the MLEnd deception dataset <br>\n",
    "**Transformation stage:** <br>\n",
    "    **-Segmentation:** Split the audio recordings into 30-second segments. The input will be the raw audio recording and the output will be multiple 30-second  with the same label of segments  as the raw audio file. A pandas dataframe will store the metadata namely the path ['X_paths'], label ['Y', 'Y_encoded'], file name ['FileID'] and the segment name['FileSegment'] <br>  \n",
    "    **-Feature Extraction:** Extract features from each segment. The input will be multiple 30 segments audio file from which four features namely power, pitch mean, pitch standard deviation and ratio of voiced frames will be extracted. The output is a stored again in the same dataframe containg the metadata from the segmentation.   <br>\n",
    "    **-Normalization/Scaling:** Normalize the extracted features using the StandardScaler function to ensure that the features have a mean of 0 and a standard deviation of 1. This makes the features comparable and prevents features with larger numerical ranges from dominating the model's learning process.<br>\n",
    "\n",
    "**Model stage:** <br>\n",
    "    **-Model Training:** Train multiple models (e.g. Logistic regression, Decision Trees, SVM, K-Nearest Neighbors (KNN)) on the transformed training data.<br>\n",
    "    **-Model Evaluation:** Evaluate the performance of individual models using metrics like accuracy, precision, recall, or F1 score on validation data<br>\n",
    "\n",
    "**Ensemble stage:** Combine the predictions from multiple models using different ensemble techniques and evaluate their performance <br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1nDXnzYLLH6"
   },
   "source": [
    "\n",
    "## 4.1 Transformation stage\n",
    "\n",
    "Initial analysis of the the audio recordings in the MLEnd deception dataset hows the recordings are beyond the 30-sec duration.\n",
    "\n",
    "**4.1.1 Segmentation:** The function **split_audio_files** is built to split the audio files into multiple segments of 30sec with an overlap of 50% ie 15 sec. The overlap ensures that the pattern at the borders are not missed as well as increasing the number of samples in the training and test set. The labels are propogated ahead from the original audio file to each of its segments. For eg for the Aduio file 00001.wav with duration 122.17 seconds the segments are as follows:   \n",
    "segment 1: 00001_01.wav 0 - 30sec &emsp;segment 2: 00001_02.wav 15 - 45sec &emsp;segment 3: 00001_03.wav 30 - 60sec  \n",
    "segment 4: 00001_04.wav 45 - 75sec &emsp;segment 5: 00001_02.wav 60 - 90sec &emsp;segment 6: 00001_03.wav 75 - 105sec   \n",
    "segment 7: 00001_07.wav 90 - 120sec &emsp;segment 8: 00001_08.wav 105 - 122.17sec (which is less than 30sec so discarded)\n",
    "\n",
    "Total 7 segments are created from the audio file 00001.wav. \n",
    "\n",
    "Similarly all the audio files in the training and test set will be split to 30sec duration and stored in the specified path. Information on these audio files will ne stored sseperately in a pandas dataframe with columns namely path ['X_paths'], label ['Y', 'Y_encoded'], file name ['FileID'] and the segment name['FileSegment'] and will be returned from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split audio files into 30-second segments with 15-second overlap\n",
    "def split_audio_files(raw_data, destination_folder, window_duration=30, overlap=15):\n",
    "    \"\"\"\n",
    "    This function splits audio files into smaller segments of fixed duration with a specified overlap.\n",
    "    The segments are saved as separate audio files in the provided destination folder. Metadata for each\n",
    "    segment (such as file path, label, and segment ID) is stored in a DataFrame and returned.\n",
    "\n",
    "    Parameters:\n",
    "    raw_data (pandas DataFrame): A DataFrame containing metadata for the audio files. \n",
    "    destination_folder (str): Path to the folder where the audio segments will be saved.\n",
    "    window_duration (int, optional): Duration (in seconds) of each segment (default is 30 seconds).\n",
    "    overlap (int, optional): Duration (in seconds) of the overlap between consecutive segments (default is 15 seconds).\n",
    "\n",
    "    Returns:\n",
    "    segment_df (pandas DataFrame): A DataFrame containing the metadata of the audio segments with File and Segment names\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(destination_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "    # Create a list to store segment details\n",
    "    segment_data = []\n",
    "\n",
    "    # Process each audio file in the DataFrame\n",
    "    for i, row in raw_data.iterrows():\n",
    "        file_path = row['X_paths']\n",
    "        label = row['Y']\n",
    "        label_encoded = row['Y_encoded']\n",
    "        fileID = f\"{i+1:05d}\"  # Unique ID for each file\n",
    "\n",
    "        # Debug prints to verify the file processing\n",
    "        print(f\"\\nProcessing file: {file_path}\")\n",
    "        print(f\"FileID: {fileID}, Label: {label}, Encoded Label: {label_encoded}\")\n",
    "\n",
    "        # Load the audio\n",
    "        x, fs = librosa.load(file_path, sr=None)  # Use original sampling rate\n",
    "        print(f\"Audio duration: {len(x) / fs:.2f} seconds\")\n",
    "\n",
    "        # Calculate window and step size in samples\n",
    "        window_length = int(window_duration * fs)\n",
    "        step_size = int((window_duration - overlap) * fs)\n",
    "\n",
    "        # Sliding window logic\n",
    "        start_index = 0\n",
    "        segment_count = 0\n",
    "\n",
    "        while start_index + window_length <= len(x):  # Only process if full window fits\n",
    "            end_index = start_index + window_length\n",
    "            segment = x[start_index:end_index]  # Extract the segment\n",
    "            start_index += step_size\n",
    "            segment_count += 1\n",
    "\n",
    "            # Create the segment file name\n",
    "            segment_name = f\"{fileID}_{segment_count:02d}.wav\"\n",
    "            segment_path = os.path.join(destination_folder, segment_name)\n",
    "            sf.write(segment_path, segment, fs)  # Save the segment to the destination folder\n",
    "\n",
    "            # Append segment details to the list\n",
    "            segment_data.append({\n",
    "                'X_paths': segment_path,\n",
    "                'Y': label,\n",
    "                'Y_encoded': label_encoded,\n",
    "                'FileID': fileID,\n",
    "                'FileSegment': f\"{fileID}_{segment_count:02d}\"\n",
    "            })\n",
    "            #print(f\"Saved segment: {segment_path}\")\n",
    "        \n",
    "        # After processing all segments for the current file, print the total number of segments\n",
    "        print(f\"Total number of segments: {segment_count}\")\n",
    "\n",
    "\n",
    "    # Create a DataFrame from the segment details\n",
    "    segment_df = pd.DataFrame(segment_data)\n",
    "    return segment_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****4.1.2 Feature Extraction**:\n",
    "\n",
    "Complex input data types such as the Audio signals has hundreds and thousands of dimensions. Therefore if we feed our model the raw input signal which has fewer samples as compared to the features then we will encounter the curse of dimentionality. Thus, we will extract some useful features and use them as predictors to train our model. They features extracted are as follows:\n",
    "\n",
    "Power: The average energy level of the audio signal, indicating the loudness of the speech.It is calculated using the formula\n",
    "\n",
    "$$\\text{Power} = \\frac{1}{N} \\sum_{i=1}^{N} x[i]^2$$\n",
    "\n",
    "Where:\n",
    "\n",
    "x[i] = Amplitude of the i-th audio sample.\n",
    "N = Total number of samples in the audio signal\n",
    "\n",
    "\n",
    "Pitch - Mean: The average frequency of the speaker's voice, reflecting the overall tone or pitch level. It is calculated using the librosa.pyin function to get the pitch of each frame and then taking its mean.\n",
    "\n",
    "Pitch - Standard Deviation: The variability in pitch over the audio, capturing fluctuations in tone or intonation. It is also calculated using the librosa.pyin function to get the pitch of each frame and then taking its standard deviation.\n",
    "\n",
    "Fraction of Voiced Region: The proportion of the audio duration containing voiced (spoken) sounds, as opposed to silence or unvoiced noise. It is also calculated using the using the binary voiced_flag array in librosa.pyin function and then taking the ratio of of voiced regions in the audio.\n",
    "\n",
    "\n",
    "The function **extract_features** and **getPitch** from the starter kit are used to extract features from the audio signal. Additionally these features are stored in a pandas.dataframe along with their corresponding labels and ID and exported as a csv file for ease of use during experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPitch(x, fs, winLen=0.02):\n",
    "    \"\"\"\n",
    "    This function estimates the pitch and voiced/unvoiced flags from an audio signal\n",
    "    Parameters:\n",
    "    x (numpy array): The input audio signal (time-domain samples).\n",
    "    fs (int): The original sample rate (sampling frequency) of the audio signal in Hz.\n",
    "    winLen (float): The window length (in seconds) for pitch estimation (default is 0.02 seconds).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Calculate the frame length in samples based on the given window length (winLen) and sampling rate (fs)\n",
    "    p = winLen * fs\n",
    "    \n",
    "    # Adjust frame_length to the nearest power of 2 greater than or equal to p\n",
    "    frame_length = int(2**int(p - 1).bit_length())\n",
    "    \n",
    "    # Set hop length to half the frame length for overlapping frames\n",
    "    hop_length = frame_length // 2\n",
    "    \n",
    "    # Use librosa's pyin function to estimate the pitch (f0), voiced/unvoiced flag, and voiced probabilities\n",
    "    # fmin and fmax specify the minimum and maximum pitch range (80 Hz to 450 Hz)\n",
    "    # sr is the sample rate, frame_length and hop_length control the time resolution of the pitch estimation\n",
    "    f0, voiced_flag, voiced_probs = librosa.pyin(y=x, fmin=80, fmax=450, sr=fs,\n",
    "                                                 frame_length=frame_length, hop_length=hop_length)\n",
    "    \n",
    "    # Return the estimated pitch (f0) and the voiced/unvoiced flags\n",
    "    return f0, voiced_flag\n",
    "\n",
    "\n",
    "def extract_features(data, scale_audio=False):\n",
    "    \"\"\"\n",
    "    This function extracts audio features (such as power, pitch mean, pitch standard deviation, and voiced fraction)\n",
    "    from a dataset of audio files.\n",
    "    Parameters:\n",
    "    data (pandas DataFrame): A DataFrame containing metadata of the audio files\n",
    "    scale_audio (bool): A flag to scale the audio signal (default is False). If True, the audio signal will be\n",
    "                        normalized to the range [-1, 1].\n",
    "    Returns:\n",
    "    feature_df (pandas DataFrame): A DataFrame containing the extracted features for each audio file along with\n",
    "                                   its metadata.\n",
    "    \"\"\"                        \n",
    "\n",
    "   \n",
    "    # Create empty lists to store the extracted features and metadata\n",
    "    features = []\n",
    "\n",
    "    # Iterate through each row in the data DataFrame\n",
    "    for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        file_path = row['X_paths']\n",
    "\n",
    "        # Load the audio file\n",
    "        x, fs = librosa.load(file_path, sr=None)  # Use the default sampling rate\n",
    "\n",
    "        if scale_audio:\n",
    "            x = x / np.max(np.abs(x))  # Scale the audio if required\n",
    "\n",
    "        # Extract pitch and voiced flag\n",
    "        f0, voiced_flag = getPitch(x, fs, winLen=0.02)\n",
    "\n",
    "        # Extract features\n",
    "        power = np.sum(x ** 2) / len(x)\n",
    "        pitch_mean = np.nanmean(f0) if np.mean(np.isnan(f0)) < 1 else 0\n",
    "        pitch_std = np.nanstd(f0) if np.mean(np.isnan(f0)) < 1 else 0\n",
    "        voiced_fr = np.mean(voiced_flag)\n",
    "\n",
    "        # Add the features and metadata as a row to the list\n",
    "        feature_row = {\n",
    "            'X_paths': row['X_paths'],\n",
    "            'Y': row['Y'],\n",
    "            'Y_encoded': row['Y_encoded'],\n",
    "            'FileID': row['FileID'],\n",
    "            'FileSegment': row['FileSegment'],\n",
    "            'power': power,\n",
    "            'pitch_mean': pitch_mean,\n",
    "            'pitch_std': pitch_std,\n",
    "            'voiced_fr': voiced_fr\n",
    "        }\n",
    "\n",
    "        features.append(feature_row)\n",
    "\n",
    "    # Create a new DataFrame from the list of features\n",
    "    feature_df = pd.DataFrame(features)\n",
    "    return feature_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data normalisation:**\n",
    "\n",
    "The extracted attributes have very different ranges and so the features with very large values can lead to giving more weight to them in the model. hence, the attributes are scaled to that they all belong to similar ranges using the StandardScalar function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0F5_kI95LuZ2"
   },
   "source": [
    "## 4.2 Model stage\n",
    "\n",
    "Supervised learning techniques: The MLEnd deception dataset contained labelled samples, therefore different supervised learning models are chosen. They are built using a dataset of labelled examples by maping the predictor attributes to their corresponding label attribute. Non parametric approaches are used as they offer flexible decision boundary and capacity to handle complex data. The following techniques are experimented\n",
    "\n",
    "\n",
    "**Support Vector Machine (SVM):**\n",
    "\n",
    "Support Vector Machines are supervised learning models used for classification task. They aim to find the optimal hyperplane (decision boundary) that separates data points into different classes with the maximum margin.The support Vectors are the data points closest to the hyperplane and helps in defining the margin. The following key hyperparameter are optimised using grid search: <br>\n",
    "-Soft Margin (C): Regularization parameter that balances margin maximization and classification error.<br>\n",
    "-Gamma: It defines how far the influence of a single training example reaches.<br>\n",
    "-Radial Basis Function (RBF) Kernel: Itenables handling of non-linear relationships effectively by mapping the features into a higher-dimensional space.<br>\n",
    "SVM is suitable for this task as they perform well in high-dimensional feature spaces, can handle non-linear relationships and robust to noise through focus on focus on support vectors.\n",
    "\n",
    "\n",
    "**K-Nearest Neighbors (KNN):**\n",
    "\n",
    "KNN is a supervised learning algorithm used for classification, which predicts the class of a data point based on the majority class of its \n",
    "k nearest neighbors in the feature space. It is chosen for its simplicity, adaptability to complex decision boundaries, and reliance on local patterns in the data for accurate classification. Key hyperparameters include:\n",
    "\n",
    "-Number of Neighbors (k): Determines how many neighbors are considered for classification.<br>\n",
    "-Distance Metric: Common metrics include Euclidean, Manhattan, or Minkowski distance, which measure proximity between data points.<br>\n",
    "-Weighting: Neighbors can be weighted uniformly or inversely proportional to their distance from the query point.<br>\n",
    "\n",
    "**Decision Tree:** is a supervised learning algorithm used for classification, which splits the data into subsets based on feature values, creating a tree-like model of decisions. It is chosen for their interpretability, ability to handle non-linear relationships, and suitability for datasets with mixed feature types. Key hyperparameters include:\n",
    "\n",
    "Max Depth: Limits the depth of the tree to control overfitting. <br>\n",
    "Min Samples Split: The minimum number of samples required to split a node. <br>\n",
    "Criterion: Determines the measure of impurity (e.g., Gini index or entropy). <br>\n",
    "Further parameters will be explored after experimentation with these\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vftsujoGIVmc"
   },
   "source": [
    "## 4.3 Ensemble stage\n",
    "\n",
    "Ensemble methods helps to create a new model that combines the strengths of diverse base models. The following ensemble techniques are used\n",
    "\n",
    "**Random Forest** is an ensemble of decision trees that builds multiple decision trees by randomising the training samples and the predictors during training. It then averages the individual predications to get predictions. Tt is an an chosen for its robustness, ability to handle high-dimensional and imbalanced datasets, and reduced risk of overfitting compared to a single decision tree. Key hyperparameters include:\n",
    "\n",
    "Number of Trees (n_estimators): The total trees in the forest. <br>\n",
    "Max Features: The number of features considered for splitting at each node. <br>\n",
    "Max Depth: Limits the depth of each tree to control complexity and overfitting. <br>\n",
    "Further parameters will be explored after experimentation with these and explained with code <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZQPxztuL9AW"
   },
   "source": [
    "# 5 Dataset\n",
    "\n",
    "The following datasets have been created to build and evaluate the models using the MLEnd deception dataset.\n",
    "\n",
    "Test Dataset: The provided dataset of 100 audio recordings are split in 80-20 ratio of train and test set. The spilt is done using stratified spiltting to ensure equal representation of both labels in the test set. This ensures that both the labels are tested equally. Once done, the test dataset is kept aside for the final testing.\n",
    "\n",
    "Train Dataset: The training datset comprises of 80 samples of the audio recording with equal number of true and deceptive labels ie 40 true and 40 deceptive stories to attain a balance dataset. This is done to provide same opportunity for our model to learn from both label categories and to avoid any bias in training our model more on one label leading to underperformance of the other label. Segmentation and feature extraction is performed after which it is further split in validation dataset.\n",
    "\n",
    "Validation Dataset: The train data is further split in a validation dataset of 20%. The validation split is done later so that we dont have to do the audio segmentation and feature extraction for the validation set again and again for different experiments. \n",
    "One limitation we have is that our train and validation set may contain segment from the same audio file leading to data leakage and bias predictions. This is avoided using grouped splitting of validation set as per file ID however due to complexity involved in grouped splitting for each fold in k-fold cross validation, the k-fold cross validation is not performed.\n",
    "\n",
    "Below we are loading the MLEnd deceptive data and creating the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Function to install packages silently\n",
    "def install_package(package):\n",
    "    subprocess.run([\"pip\", \"install\", package], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "# Install the packages\n",
    "install_package(\"mlend==1.0.0.4\")\n",
    "install_package(\"pydub\")\n",
    "install_package(\"librosa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import re\n",
    "import pickle\n",
    "import glob\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pydub\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from scipy.io import wavfile\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report,confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import mlend\n",
    "from mlend import download_deception_small, deception_small_load\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Define the destination folder\n",
    "destination_folder = os.path.join(current_directory, \"MLEnd\")\n",
    "subdirectory = 'MLEnd/deception'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "#datadir = download_deception_small(save_to=destination_folder, subset={}, verbose=1, overwrite=False) #to download data\n",
    "datadir = os.path.join(current_directory, subdirectory) #to use already download data in the current directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "DataSet, _, MAPs = deception_small_load(datadir_main=datadir, train_test_split=None, verbose=1, encode_labels=True) #Loading the whole dataset without splitting in train and test test\n",
    "\n",
    "# Print the data types of the loaded data\n",
    "print(f\"Datatype of DataSet: {type(DataSet)}\")\n",
    "print(f\"Datatype of MAPs: {type(MAPs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=ImportWarning)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Converting the dictionary to a pandas DataFrame\n",
    "DataSet = pd.DataFrame(DataSet).sort_values(by='X_paths', ascending=True)\n",
    "DataSet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided dataset contains the following details <br>\n",
    "'X_paths': corresponds to the location of the audio file<br>\n",
    "'Y': story type (true or deceptive)<br>\n",
    "'Y_encoded': binary label for the story type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Test Dataset\n",
    "\n",
    "Building the Test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset in train and test sets before building the pipeline to prevent any data leakages\n",
    "\n",
    "# Separate labels for a stratified split\n",
    "X = DataSet.drop(columns=[\"Y_encoded\"])\n",
    "y = DataSet[\"Y_encoded\"]\n",
    "\n",
    "# Perform stratified split for a balanced train and set spilt\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=22)\n",
    "\n",
    "# Combine X and y back into DataFrames\n",
    "TrainData = pd.concat([X_train, y_train], axis=1)\n",
    "TestData = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Resetting the index for both train and test sets\n",
    "#TrainData = TrainData.reset_index(drop=True)\n",
    "#TestData = TestData.reset_index(drop=True)\n",
    "\n",
    "# No of samples in TrainData\n",
    "train_counts = Counter(TrainData['Y_encoded'])\n",
    "print(f\"TrainData - Number of true stories: {train_counts[0]}, Number of deceptive stories: {train_counts[1]}\")\n",
    "\n",
    "# No of samples in TestData\n",
    "test_counts = Counter(TestData['Y_encoded'])\n",
    "print(f\"TestData - Number of true stories: {test_counts[0]}, Number of deceptive stories: {test_counts[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Train Dataset\n",
    "After the split, the train set comprises of 80 samples in the TrainData which we can view below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DataFrame\n",
    "TrainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing the first 5 audio files in TrainData\n",
    "for idx, row in TrainData.head(5).iterrows():  # iterating directly over the first 5 rows\n",
    "    audio_path = row['X_paths']  # Accessing the path from the row\n",
    "    print(f\"Audio file path: {audio_path}\")\n",
    "    \n",
    "    # Accessing the label\n",
    "    audio_label = row['Y']\n",
    "    \n",
    "    # Display the audio\n",
    "    ipd.display(ipd.Audio(audio_path))\n",
    "    \n",
    "    # Displays the file name and label of the played audio\n",
    "    file_name = audio_path.split('/')[-1]\n",
    "    print(f\"File name of the played audio: {file_name}\")\n",
    "    print(f\"Label of the played audio: {audio_label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the duration of the audio recordings is greater than the required 30-sec therefore we will use the split_audio_files function to extract 30-sec segments with a 50% overlap and save it in a seperate folder named 'train_segments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = os.path.join(current_directory, \"train_segments\")\n",
    "\n",
    "# Split and save segments for the training data\n",
    "TrainSegments = split_audio_files(TrainData, train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Display the resulting DataFrame\n",
    "print(\"\\nSegmented DataFrame:\")\n",
    "TrainSegments.iloc[:, 0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the balance of dataset\n",
    "\n",
    "# Count of true and deceptive stories\n",
    "counts = TrainSegments['Y_encoded'].value_counts()\n",
    "count_ones = counts[1] if 1 in counts else 0\n",
    "count_zeros = counts[0] if 0 in counts else 0\n",
    "\n",
    "# Total count\n",
    "total_count = count_ones + count_zeros\n",
    "\n",
    "# Calculate Percentages\n",
    "percentage_ones = (count_ones / total_count) * 100 if total_count > 0 else 0\n",
    "percentage_zeros = (count_zeros / total_count) * 100 if total_count > 0 else 0\n",
    "\n",
    "print(f\"\\nTotal segments: {total_count}\")\n",
    "print(f\"Number of segments of true stories: {count_zeros} ({percentage_zeros:.2f}%)\")\n",
    "print(f\"Number of segments deceptive stories: {count_ones} ({percentage_ones:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the classes is relatively close to 50:50. Hence no significant issue for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and create a new DataFrame\n",
    "TrainSegments_with_features = extract_features(TrainSegments, scale_audio=True)\n",
    "\n",
    "# Define the CSV file path in the current working directory\n",
    "csv_path = os.path.join(current_directory, 'TrainSegments_with_features.csv')\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "TrainSegments_with_features.to_csv(csv_path, index=False)\n",
    "#print(f\"DataFrame saved to {csv_path}\")\n",
    "\n",
    "# Read the CSV file from the current working directory\n",
    "TrainSegments_with_features = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the new DataFrame\n",
    "TrainSegments_with_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the features for the pairplot\n",
    "features = TrainSegments_with_features.iloc[:, 5:9]\n",
    "\n",
    "# Adding the label column\n",
    "features['Y_encoded'] = TrainSegments_with_features['Y_encoded']\n",
    "\n",
    "# Creating the pairplot\n",
    "sns.pairplot(features, hue='Y_encoded')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots show significant overlap in all the features however there are some slight dense clusters for each of the labels which hopefully will be picked up by the model. The true label histogram(diagonal matrix) for each feature is more than the deceptive due to the slight imbalance after splitting the recordings in segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Validaton Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data in validation set\n",
    "\n",
    "# Extract features, labels, and FileID column\n",
    "X = TrainSegments_with_features[['power', 'pitch_mean', 'pitch_std', 'voiced_fr']]\n",
    "y = TrainSegments_with_features['Y_encoded']\n",
    "groups = TrainSegments_with_features['FileID']  # Group by FileID to maintain separation\n",
    "\n",
    "# Initialize GroupShuffleSplit\n",
    "group_splitter = GroupShuffleSplit(test_size=0.2, random_state=22)\n",
    "\n",
    "# Split data into train and validation sets based on FileID\n",
    "train_idx, val_idx = next(group_splitter.split(X, y, groups=groups))\n",
    "\n",
    "# Create training and validation sets\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "# # Extract FileID for verification\n",
    "# train_file_ids = TrainSegments_with_features.iloc[train_idx]['FileID'].unique()\n",
    "# val_file_ids = TrainSegments_with_features.iloc[val_idx]['FileID'].unique()\n",
    "\n",
    "# Check if there's any overlap between the train and validation FileIDs\n",
    "# overlap = set(train_file_ids).intersection(set(val_file_ids))\n",
    "\n",
    "# # Print verification result\n",
    "# if len(overlap) == 0:\n",
    "#     print(\"The train and validation sets contain data from different FileIDs.\")\n",
    "# else:\n",
    "#     print(f\"Overlap found between train and validation sets for FileIDs: {overlap}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qf7GN1aeXJI"
   },
   "source": [
    "# 6 Experiments and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed data of the Train Dataset will be fed to the models and use the validation data to evaluate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Generate and plot confusion matrices: one with counts and one with ratios.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): True labels.\n",
    "    y_pred (array-like): Predicted labels.\n",
    "    \"\"\"\n",
    "    # Generate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Calculate the ratio matrix (normalize by column sum)\n",
    "    conf_matrix_ratio = conf_matrix.astype('float') / conf_matrix.sum(axis=0)\n",
    "\n",
    "    # Subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Plot the count confusion matrix (Actual as columns, Predicted as rows)\n",
    "    sns.heatmap(conf_matrix.T, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['True (0)', 'Deceptive (1)'], \n",
    "                yticklabels=['True (0)', 'Deceptive (1)'], \n",
    "                ax=axes[0])\n",
    "    axes[0].set_title('Confusion Matrix (Count)')\n",
    "    axes[0].set_xlabel('Actual')\n",
    "    axes[0].set_ylabel('Predicted')\n",
    "\n",
    "    # Plot the ratio confusion matrix (Actual as columns, Predicted as rows)\n",
    "    sns.heatmap(conf_matrix_ratio.T, annot=True, fmt='.2f', cmap='Greens', \n",
    "                xticklabels=['True (0)', 'Deceptive (1)'], \n",
    "                yticklabels=['True (0)', 'Deceptive (1)'], \n",
    "                ax=axes[1])\n",
    "    axes[1].set_title('Confusion Matrix (Ratio)')\n",
    "    axes[1].set_xlabel('Actual')\n",
    "    axes[1].set_ylabel('Predicted')\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val) #scale the validation data using the mean and standard deviation values calculated from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='rbf',class_weight='balanced')\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = svm_model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can we seen, the model has accuracy is 0.5 ie 50% similar to tossing a coin where is probability of predicting a story as true or deceptive is 50-50 percent and hence we need to improve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM model with hyperparameter tuning\n",
    "svm_model = SVC(C=1, gamma=2, kernel='rbf',class_weight='balanced', probability=True)#using the hyperparameters are given in the starter kit\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = svm_model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
    "plot_confusion_matrices(y_val, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model accuracy has improved slightly with an overall accuracy of 53%. However it tends to be performing better for the true (0) label over deceptive class (1) as seen in its higher precision (0.59 vs. 0.40) and recall (0.65 vs. 0.35). F1-score for true label indicate a balanced performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Parameter grid for KNN\n",
    "param_dist_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 10],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "# Initialize KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Initialize RandomizedSearchCV for KNN with 3-fold cross-validation\n",
    "random_search_knn = RandomizedSearchCV(estimator=knn_model, param_distributions=param_dist_knn, \n",
    "                                       n_iter=10, cv=3, n_jobs=-1, random_state=22)\n",
    "\n",
    "# Fit the RandomizedSearchCV\n",
    "random_search_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and print them\n",
    "best_params_knn = random_search_knn.best_params_\n",
    "print(f\"\\nBest KNN Parameters: {best_params_knn}\")\n",
    "\n",
    "# KNN Model Evaluation\n",
    "best_knn_model = random_search_knn.best_estimator_\n",
    "y_pred_knn = best_knn_model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate KNN\n",
    "accuracy_knn = accuracy_score(y_val, y_pred_knn)\n",
    "print(\"\\nKNN Accuracy:\", accuracy_knn)\n",
    "print(\"\\nKNN Classification Report:\\n\", classification_report(y_val, y_pred_knn))\n",
    "\n",
    "plot_confusion_matrices(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN model achieves an accuracy of 49.6% which is also better identifying class 0 (precision: 0.57, recall: 0.60) compared to class 1 (precision: 0.37, recall: 0.35)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=22)\n",
    "\n",
    "# Train the Decision Tree model\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_dt = dt_model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_dt = accuracy_score(y_val, y_pred_dt)\n",
    "print(\"\\nDecision Tree Accuracy:\", accuracy_dt)\n",
    "print(\"\\nDecision Tree Classification Report:\\n\", classification_report(y_val, y_pred_dt))\n",
    "plot_confusion_matrices(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the The Decision Tree model achieves the highest accuracy of 59.5%. It is also showing better performance in predicting class 0 (precision: 0.65, recall: 0.69) compared to class 1 (precision: 0.50, recall: 0.45). However they tends to overfit hence we are using random forest as an ensemble of decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Random Forest. Its an ensemble of decision trees.\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=22)\n",
    "random_search_rf = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist_rf, \n",
    "                                      n_iter=10, cv=5, n_jobs=-1, random_state=22)\n",
    "\n",
    "random_search_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Random Forest Model Evaluation\n",
    "best_rf_model = random_search_rf.best_estimator_\n",
    "y_pred_rf = best_rf_model.predict(X_val_scaled)\n",
    "accuracy_rf = accuracy_score(y_val, y_pred_rf)\n",
    "print(\"\\nRandom Forest Accuracy:\", accuracy_rf)\n",
    "print(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_val, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest model achieves an accuracy of 50.4% which again is not a good model. Tuning the hyperparameters by using randomized grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter space \n",
    "param_dist_rf = {\n",
    "    'n_estimators': [100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [10, 20, None],  # Maximum depth of the trees (None means no limit)\n",
    "    'min_samples_split': [2, 5],  # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2],   # Minimum samples required to be at a leaf node\n",
    "    'max_features': ['sqrt', 'log2'],  # Number of features to consider for splitting at each node (sqrt and log2 are common options)\n",
    "    'bootstrap': [True, False],   # Whether to sample the data with or without replacement (True for bootstrapping)\n",
    "    'class_weight': [None, 'balanced']  # Weigh classes inversely proportional to class frequencies (useful for imbalanced data)\n",
    "}\n",
    "\n",
    "# Sample 50% of the data for quick tuning\n",
    "X_tune, _, y_tune, _ = train_test_split(X_train_scaled, y_train, test_size=0.5, random_state=22)\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=22)\n",
    "\n",
    "# RandomizedSearchCV for faster hyperparameter tuning\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=rf_model, \n",
    "    param_distributions=param_dist_rf, \n",
    "    n_iter=20,  # Number of combinations to try\n",
    "    cv=3,       # No of folds for cross-validation\n",
    "    scoring='roc_auc',  # Use ROC-AUC as evaluation metric\n",
    "    random_state=22,\n",
    "    n_jobs=-1,  # Use all CPU cores for parallel processing\n",
    "    verbose=2   # Display progress\n",
    ")\n",
    "\n",
    "# Fit the model using a subset of the training data\n",
    "random_search_rf.fit(X_tune, y_tune)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_rf_model = random_search_rf.best_estimator_\n",
    "print(\"\\nBest Parameters:\", random_search_rf.best_params_)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_rf = best_rf_model.predict(X_val_scaled)\n",
    "y_proba_rf = best_rf_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_rf = accuracy_score(y_val, y_pred_rf)\n",
    "roc_auc_rf = roc_auc_score(y_val, y_proba_rf)\n",
    "print(\"\\nImproved Random Forest Accuracy:\", accuracy_rf)\n",
    "print(\"\\nImproved Random Forest ROC-AUC:\", roc_auc_rf)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ensembling techniques were also experimented such as stacking with SVM and KNN as basemodel and Logistic Regression as meta-model, Voting , Boosting using AdaBoost with SVM, AdaBoost with Decision Tree as the base estimator but not significant improvements achieved hence not included in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Testing\n",
    "\n",
    "The final testing is done on Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Running the test data through the transformation stage\n",
    "test_folder = os.path.join(current_directory, \"test_segments\")\n",
    "csv_test_path = os.path.join(current_directory, 'TestSegments_with_features.csv')\n",
    "\n",
    "TestSegments = split_audio_files(TestData, test_folder)\n",
    "TestSegments_with_features = extract_features(TestSegments, scale_audio=True)\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "TestSegments_with_features.to_csv(csv_test_path, index=False)\n",
    "\n",
    "# Read the CSV file from the current working directory\n",
    "TestSegments_with_features = pd.read_csv(csv_test_path)\n",
    "TestSegments_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = TestSegments_with_features[['power', 'pitch_mean', 'pitch_std', 'voiced_fr']]\n",
    "y_test = TestSegments_with_features['Y_encoded']\n",
    "\n",
    "# Standardize the features (same scaling as during training)\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "# Predict with the trained random forest classifier\n",
    "y_pred_rf_test = best_rf_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the test model\n",
    "accuracy_rf_test = accuracy_score(y_test, y_pred_rf_test)\n",
    "print(\"\\n Deception Model Accuracy using Random Forest:\", accuracy_rf_test)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf_test))\n",
    "plot_confusion_matrices(y_test, y_pred_rf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSrJCR_cekPO"
   },
   "source": [
    "# 7 Conclusions\n",
    "\n",
    "The final evaluation is done on the reserved unseen test data and the following quality metrics were obtained.\n",
    "A low accuracy of 34.3%, with similar but weak performance for both classes (class 0: precision 0.37, recall 0.32; class 1: precision 0.32, recall 0.37). The F1-scores of 0.34 for both classes indicate that the model struggles to effectively differentiate between them.\n",
    "\n",
    "Conclusion\n",
    "- The model doesnot seem to generalise well as the performance was slightly better on the training and validation set. However, considerable low performance on the unseen test set of 0.3. If we perform a not function on our predications, we will end up with better results. \n",
    "\n",
    "Suggestions for improvements:\n",
    "- Enhanced features: Include additional audio features such as speech rate, Mel-frequency cepstral coefficients (MFCCs),\n",
    "- Advance Models: Use of deep neural networks to capture complex patterns. Further we can utilize transfer learning with pretrained models such as DeepSpeech or Wav2Vec 2.0.\n",
    "- Cross validation: Use K-fold cross validation to ensure robust model performance\n",
    "- Hyperparameter optimization: Utilize wide range of grid search or random search for hyperparameter tuning to identify the optimal parameters for the model\n",
    "- By increasing the amount of training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlGWHKQFIVmc"
   },
   "source": [
    "# 8 References\n",
    "\n",
    "Acknowledge others here (books, papers, repositories, libraries, tools)\n",
    "\n",
    "Articles:\n",
    "\n",
    "https://towardsdatascience.com/understanding-audio-data-fourier-transform-fft-spectrogram-and-speech-recognition-a4072d228520\n",
    "\n",
    "https://medium.com/analytics-vidhya/audio-data-processing-feature-extraction-science-concepts-behind-them-be97fbd587d8\n",
    "\n",
    "https://medium.com/analytics-vidhya/audio-data-processing-feature-extraction-essential-science-concepts-behind-them-part-2-9c738e6a7f99\n",
    "\n",
    "https://daehnhardt.com/blog/2023/03/05/python-audio-signal-processing-with-librosa/\n",
    "\n",
    "https://wiki.cci.arts.ac.uk/books/how-to-guides/page/audio-files-with-librosa\n",
    "\n",
    "repositories:\n",
    "https://github.com/alicex2020/Deep-Learning-Lie-Detection\n",
    "\n",
    "https://github.com/m-shahbaz-kharal/py_lie_detect\n",
    "\n",
    "https://github.com/craigfrancis/audio-detect\n",
    "\n",
    "https://github.com/librosa/librosa\n",
    "\n",
    "https://github.com/hernanrazo/human-voice-detection\n",
    "\n",
    "https://github.com/Ribin-Baby/Audio-Processing\n",
    "\n",
    "Libraries:\n",
    "\n",
    "Python Software Foundation. (n.d.). Python Language Reference. Retrieved from https://www.python.org/\n",
    "\n",
    "NumPy Developers. (2024). NumPy: The fundamental package for scientific computing with Python. Retrieved from https://numpy.org/\n",
    "\n",
    "Pandas Development Team. (2024). Pandas: Python Data Analysis Library. Retrieved from https://pandas.pydata.org/\n",
    "\n",
    "Hunter, J. D. (2007). Matplotlib: A 2D graphics environment. IEEE Xplore, DOI: 10.1109/MCSE.2007.55\n",
    "\n",
    "Waskom, M. L. (2021). Seaborn: statistical data visualization. Retrieved from https://seaborn.pydata.org/\n",
    "\n",
    "PyDub Contributors. (2024). PyDub: Python library for audio processing. Retrieved from https://pydub.com/\n",
    "\n",
    "Librosa Contributors. (2024). Librosa: Python package for music and audio analysis. Retrieved from https://librosa.org/\n",
    "\n",
    "Soundfile Contributors. (2024). Soundfile: Read and write sound files. Retrieved from https://pysoundfile.readthedocs.io/en/latest/\n",
    "\n",
    "SciPy Contributors. (2024). SciPy: Open source scientific tools for Python. Retrieved from https://scipy.org/\n",
    "\n",
    "IPython Development Team. (2024). IPython: A rich interactive environment for Python. Retrieved from https://ipython.org/\n",
    "\n",
    "TQDM Contributors. (2024). TQDM: A fast, extensible progress bar for loops and other operations. Retrieved from https://tqdm.github.io/\n",
    "\n",
    "Scikit-learn Contributors. (2024). Scikit-learn: Machine Learning in Python. Retrieved from https://scikit-learn.org/\n",
    "\n",
    "MLend Contributors. (2024). MLend: A library for machine learning in deception detection. Retrieved from https://mlend.readthedocs.io/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5Yu7OFghzpg"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
